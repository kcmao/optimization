# CNN
2. 全连接/1x1卷积的计算原理  
```
全连接其实完全就是一个参数矩阵W乘以一个输入向量X的操作gemv。如果输入大于一个batch,就是gemm:W [X1,X2],其中W是矩阵， X1,X2分别是向量。而1x1卷积其实和全连接一样，也是完全相当于gemm。
```

6. **卷积操作的本质**？  
``` 
    a. 从全连接到卷积神经网络的区别与联系来看的话，如何解释卷积：减少参数量/降低复杂度/防止过拟合。  
    b. 从图像提取特征来看的话，卷积相当于一种滤波器，通过在图像上滑动滤波。 
```

**从不同维度看卷积，卷积到底是个啥玩意儿**？    

```
1,有人说卷积相当于求解一个特征向量集。  
[](https://blog.csdn.net/weixin_42078618/article/details/83895555)  
   原话：为了卷积神经网络中，第一步一般用卷积核去提取特征，这些初始化的卷积核会在反向传播的过程中，在迭代中被一次又一次的更新，无限地逼近我们的真实解。其实本质没有对图像矩阵求解，而是初始化了一个符合某种分布的特征向量集，然后在反向传播中无限更新这个特征集，让它能无限逼近数学中的那个概念上的特征向量，以致于我们能用特征向量的数学方法对矩阵进行特征提取。

2,有人说卷积对应于一种变换算子，相当于那种旋转平移等算子，不过就是这种变换的参数是自己迭代求出来的。  

3,卷积的特点：
    稀疏交互  
    参数共享
    等变表示 ？
    具体要看花书，上面有写。  
     
```
关于卷积为啥在图像浅层提取局部信息。深层提取全局的语义信息，还是没有搞明白。