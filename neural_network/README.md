# Learning notes  
一些乱七八糟的机器学习知识：  
1. 多层感知机是叫mlp  

2. 逻辑回归和多层感知机？   
```
逻辑回归是一层感知机带上一个激活函数
```   

3. 逻辑回归和线性回归都叫做线性模型   

4. 逻辑回归/线性回归/感知机/全连接层的区别与联系？
```  
   我觉得是及其相似的产物，是不同时间段的产物，其都是线性模型y=wx+b基础上加上一个激活函数或者不同损失函数的构成物，逻辑回归/线性回归/感知机是单独作为一个算法，用于分类或者回归，全连接是在卷积出来后作为其中一层。
``` 
5. 全连接/1x1卷积的计算原理  
```
全连接其实完全就是一个参数矩阵W乘以一个输入向量X的操作gemv。如果输入大于一个batch,就是gemm:W [X1,X2],其中W是矩阵， X1,X2分别是向量。而1x1卷积其实和全连接一样，也是完全相当于gemm。
```

6. **卷积操作的本质**？  
``` 
    a. 从全连接到卷积神经网络的区别与联系来看的话，如何解释卷积：减少参数量/降低复杂度/防止过拟合。  
    b. 从图像提取特征来看的话，卷积相当于一种滤波器，通过在图像上滑动滤波。 
```

**从不同维度看卷积，卷积到底是个啥玩意儿**？    

```
1,有人说卷积相当于求解一个特征向量集。  
[](https://blog.csdn.net/weixin_42078618/article/details/83895555)  
   原话：为了卷积神经网络中，第一步一般用卷积核去提取特征，这些初始化的卷积核会在反向传播的过程中，在迭代中被一次又一次的更新，无限地逼近我们的真实解。其实本质没有对图像矩阵求解，而是初始化了一个符合某种分布的特征向量集，然后在反向传播中无限更新这个特征集，让它能无限逼近数学中的那个概念上的特征向量，以致于我们能用特征向量的数学方法对矩阵进行特征提取。

2,有人说卷积对应于一种变换算子，相当于那种旋转平移等算子，不过就是这种变换的参数是自己迭代求出来的。  

3,卷积的特点：
    稀疏交互  
    参数共享
    等变表示 ？
    具体要看花书，上面有写。  
     
```
关于卷积为啥在图像浅层提取局部信息。深层提取全局的语义信息，还是没有搞明白。

7. relu/sigmod/softmax/交叉熵等的区别   
[activation](./activation_loss.md)
```
一方面：sigmod和rulu都是激活函数，用在神经网络的后面起到非线性的作用。

另一方面：relu的出现是完全为了从激活函数的角度改进而提出的。
sigmod不经能作为非线性的激活作用，同时由于其是将输出压缩到0-1之间，更有概率的味道。和softmax一样成为一种输出数据到概率的映射作用。 

所以sigmod不仅可以作为卷积的激活部分，同时可以作为最后输出到概率的映射，和softmax一样。   

交叉熵作为一种计算损失的一个公式。用来衡量预测的输出和真值的区别。
```

继续relu,relu的稀疏作用？对量化的影响
```
relu能够将输出小于0的都变成0，使得输出很稀疏，然后下一层就很稀疏。越来越稀疏，不知道作用是什么，优缺点是什么？  

relu的存在使得量化的方式有所不同，
一般量化分为权值量化和激活值量化，如果激活函数是relu，那么直接量化到0-255，加入激活值是sigmod或其他呢，使得输出可以小于0，那么量化的方式会不会又不一样了。
```

## TODO.
### 机器学习
1. 

### 深度学习
1. BN/GN等等正则化方法。
2. 目标检测的
3. attention机制的
4. 上采样的几种：  
    反卷积()  
    upsample()  
    unpooling()  
    空洞卷积()  

5. 

### 评价指标相关
1. roc/auc/f1 score/  
   图像：  
   iou/miou/map/miss rate-fppi  






